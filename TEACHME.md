# Syllabus

To understand this code, and to build something that might be
better  than it, you need to know:

Fairness: 
- Why
- with great power comes  great responsibilty

Cognitive:
- STM, LTM, ?MRI
- heuristcs, FFTtrees

Keys:
- evident 

Learning:
- nature of  data  (x,y)
- kinds of learning problems
- kinds of  data. variance, entropy
- numerical methods: per, Some, incremental sd
- instance selection: cluster+select
- feature  selection: entropy
- discretization: MDL, chi  merge
  https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1173?casa_token=057-OyFVxmIAAAAA:ZrAMysNoVwcgYn8qr3HkcqECBQq24vfH4xOGI8ngDS2uQKKJEeNjgKk74KLSWZFaYcgm-P5GoZyNBCZP
- clustering: PDDP, random projections  (FASTMAP, gaussian trick),
  kd-trees, supervised  clustering
- Bayes fast
- active learning: Zhe

Optimization. 
- connection to learning
- landscape, local maximia.
- domination
- parametric  optimization: Simplex, Hessien
- non-parametric: SA, GA, WSat, DE, NSGA-II, MOEA/D, DODGE
- sequential model optimization:  TPE, FLASH 
- HV, spread, GD, IGB

Statistics:
- parametric, non-parametric
- effect size significance tests
- scott knott
-  bootstreap, cliffs

